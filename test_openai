# test_cogguard_international.py
# ä½¿ç”¨ OpenAI GPT-4o å¤šæ¨¡æ€ APIï¼ˆæ”¯æŒå›¾åƒ+æ–‡æœ¬è¾“å…¥ï¼‰
# éœ€è¦ï¼špip install openai python-dotenv

import json
import os
import time
import base64
from pathlib import Path
from dotenv import load_dotenv
import openai

# ==================== é…ç½®åŒº ====================
load_dotenv()  # ä» .env æ–‡ä»¶è¯»å– API_KEY
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # â† åœ¨ .env ä¸­å¡«å†™ï¼šOPENAI_API_KEY=your-key-here
MODEL = "gpt-4o"  # æ”¯æŒ gpt-4o, gpt-4-turbo
DATA_FILE = "cogguard_bench_v2.json"
IMAGE_DIR = "images"
OUTPUT_DIR = "results/international"
SLEEP_DELAY = 1.5  # é¿å…é€Ÿç‡é™åˆ¶ï¼ˆGPT-4o é™åˆ¶çº¦ 3 RPMï¼‰

# åˆ›å»ºè¾“å‡ºç›®å½•
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
openai.api_key = OPENAI_API_KEY

# ==================== è¾…åŠ©å‡½æ•°ï¼šå°†å›¾ç‰‡è½¬ä¸º Base64 ====================
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# ==================== æ„å»ºæç¤ºè¯å‡½æ•° ====================
def build_prompt(current_img_path: str, ref_img_path: str, rule: str, action_plan: List[str], env: Dict, context_level: str) -> list:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡å±‚çº§æ„å»ºå¤šæ¨¡æ€æç¤ºï¼ˆç”¨äºGPT-4oï¼‰
    :param context_level: 'A', 'B', 'C', 'D', 'E'
    :return: messages åˆ—è¡¨ï¼Œä¾› openai.ChatCompletion.create ä½¿ç”¨
    """
    base_message = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€åå®‰å…¨å·¡é€»æœºå™¨ç‹—ã€‚è¯·æ ¹æ®å›¾åƒã€è§„åˆ™ã€ç¯å¢ƒå’Œé¢„æ¡ˆï¼Œè¿›è¡Œä¸¥è°¨æ¨ç†ï¼Œè¾“å‡ºä¸¥æ ¼JSONæ ¼å¼ã€‚"
        }
    ]

    user_content = []

    # æ·»åŠ å½“å‰å›¾åƒï¼ˆæ‰€æœ‰ç»„éƒ½åŒ…å«ï¼‰
    current_base64 = encode_image(current_img_path)
    user_content.append({
        "type": "image_url",
        "image_url": {"url": f"data:image/jpeg;base64,{current_base64}"}
    })

    # æ„å»ºæ–‡æœ¬æç¤º
    text_parts = []

    if context_level in ['B', 'C', 'D', 'E']:
        text_parts.append(f"å®‰å…¨è§„åˆ™ï¼š{rule}")

    if context_level in ['C', 'D', 'E']:
        text_parts.append(f"æ­£å¸¸å‚è€ƒçŠ¶æ€ï¼ˆID: {Path(ref_img_path).stem}ï¼‰ï¼š[å›¾åƒå·²æä¾›]")

    if context_level in ['D', 'E']:
        actions_str = "\n".join([f"- {act}" for act in action_plan])
        text_parts.append(f"å¯é€‰è¡ŒåŠ¨é¢„æ¡ˆï¼š\n{actions_str}")

    if context_level == 'E':
        env_info = (
            f"ç¯å¢ƒä¿¡æ¯ï¼š\n"
            f"- ä½ç½®ï¼š{env['gps']}\n"
            f"- æ—¶é—´ï¼š{env['timestamp']}\n"
            f"- å…‰ç…§ï¼š{env['lighting']}\n"
            f"- å¤©æ°”ï¼š{env['weather']}\n"
            f"- æ˜¯å¦å·¥ä½œæ—¶é—´ï¼š{'æ˜¯' if env['is_work_hours'] else 'å¦'}\n"
            f"- ä¸Šæ¬¡å·¡é€»ï¼š{env['last_patrol_time']}"
        )
        text_parts.append(env_info)

    if not text_parts:
        text_parts.append("è¯·åˆ¤æ–­è¿™å¼ å›¾ç‰‡ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•å®‰å…¨éšæ‚£ï¼Ÿ")

    user_content.append({
        "type": "text",
        "text": "\n\n".join(text_parts) + "\n\nè¯·é€æ­¥æ¨ç†å¹¶è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼š\n{\n  \"anomaly_detected\": true/false,\n  \"reason\": \"è¯¦ç»†è§£é‡Šï¼Œå¿…é¡»å¼•ç”¨è§„åˆ™å’Œå‚è€ƒå›¾\",\n  \"action\": \"ä»é¢„æ¡ˆä¸­é€‰æ‹©çš„å”¯ä¸€è¡ŒåŠ¨\",\n  \"confidence\": 0.0â€“1.0\n}"
    })

    return base_message + [{"role": "user", "content": user_content}]

# ==================== è®¡ç®— CRS å‡½æ•°ï¼ˆä¸å›½å†…ç‰ˆå®Œå…¨ä¸€è‡´ï¼‰====================
def compute_crs(response: Dict, ground_truth: Dict) -> float:
    accuracy = 1.0 if response.get("anomaly_detected") == ground_truth["expected_anomaly"] else 0.0

    logic_base = ground_truth["expert_logic_score"]

    # ç¯å¢ƒä¿®æ­£å› å­ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆç†ä½¿ç”¨äº†ç¯å¢ƒä¿¡æ¯
    reason_lower = response.get("reason", "").lower()
    env_keywords = ["time", "hour", "night", "rain", "weather", "gps", "location", "patrol", "work hours"]
    has_env_mention = any(kw in reason_lower for kw in env_keywords)

    # å¦‚æœæ˜¯éœ€è¦ç¯å¢ƒåˆ¤æ–­çš„ç±»å‹ï¼Œä¸”æ¨¡å‹æåˆ°äº†ç¯å¢ƒ â†’ åŠ åˆ†
    if ground_truth["type"] in ["ambiguous_object_with_context", "false_positive_confounder", "rule_violation_with_confounding_env"]:
        if has_env_mention and response["anomaly_detected"] == ground_truth["expected_anomaly"]:
            logic_final = min(1.0, logic_base + 0.1)
        elif has_env_mention and response["anomaly_detected"] != ground_truth["expected_anomaly"]:
            logic_final = max(0.0, logic_base - 0.2)
        else:
            logic_final = logic_base
    else:
        logic_final = logic_base

    action_plan = ground_truth["action_plan"]
    predicted_action = response.get("action", "")
    action_score = 1.0 if any(predicted_action.strip() in act for act in action_plan) else 0.0

    confidence = response.get("confidence", 0.5)
    crs = (accuracy * 0.3 + logic_final * 0.5 + action_score * 0.2) * confidence
    return round(crs, 4)

# ==================== ä¸»ç¨‹åºï¼šäº”ç»„æ¶ˆèå®éªŒ ====================
def run_ablation_experiment():
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    context_levels = [
        ("A (Baseline)", False, False, False, False),
        ("B (+Text)", True, False, False, False),
        ("C (+Visual)", True, True, False, False),
        ("D (+Action)", True, True, True, False),
        ("E (+Env)", True, True, True, True)
    ]

    results = []

    for test_case in data:
        current_img = test_case["current_img"]
        ref_img = test_case["ref_img"]
        rule = test_case["rule_text"]
        actions = test_case["action_plan"]
        env = test_case["environment"]

        for level_name, use_text, use_ref, use_action, use_env in context_levels:
            try:
                print(f"[{level_name}] Processing {test_case['scene_id']}...")
                messages = build_prompt(current_img, ref_img, rule, actions, env, level_name[0])

                response = openai.ChatCompletion.create(
                    model=MODEL,
                    messages=messages,
                    max_tokens=500,
                    temperature=0.1,
                    top_p=0.9
                )

                content = response.choices[0].message.content.strip()

                # è§£æ JSONï¼ˆå®¹é”™å¤„ç†ï¼‰
                import re
                json_match = re.search(r'({.*})', content, re.DOTALL)
                if not json_match:
                    raise ValueError(f"æ— æ³•è§£æå“åº”ä¸ºJSON: {content[:200]}...")

                llm_output = json.loads(json_match.group(1))

                # å¼ºåˆ¶è½¬æ¢å­—æ®µï¼ˆé¿å…ç±»å‹é”™è¯¯ï¼‰
                llm_output["anomaly_detected"] = bool(llm_output.get("anomaly_detected", False))
                llm_output["confidence"] = float(llm_output.get("confidence", 0.5))
                llm_output["action"] = str(llm_output.get("action", ""))

                crs = compute_crs(llm_output, test_case)

                result = {
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": llm_output,
                    "crs": crs,
                    "raw_response": content,
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"],
                        "type": test_case["type"]
                    }
                }
                results.append(result)

                print(f"âœ… Success: {test_case['scene_id']} | {level_name} | CRS={crs}")
                time.sleep(SLEEP_DELAY)

            except Exception as e:
                print(f"âŒ Error on {test_case['scene_id']} ({level_name}): {e}")
                results.append({
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": {"error": str(e)},
                    "crs": 0.0,
                    "raw_response": "",
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"],
                        "type": test_case["type"]
                    }
                })

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, "results.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å›½é™…ç‰ˆå®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")

    # æ±‡æ€»ç»Ÿè®¡
    from collections import defaultdict
    crs_by_level = defaultdict(list)
    for r in results:
        crs_by_level[r["context_level"]].append(r["crs"])

    print("\nğŸ“Š å›½é™…ç‰ˆ CRSSummaryï¼š")
    for level, scores in crs_by_level.items():
        avg = sum(scores) / len(scores)
        print(f"{level}: {avg:.4f} (n={len(scores)})")

if __name__ == "__main__":
    run_ablation_experiment()