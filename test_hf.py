# test_cogguard_china.py
# ä½¿ç”¨ Qwen2-VL-7B æœ¬åœ°æ¨¡å‹ï¼ˆå›½å†…å¯ç”¨ï¼Œæ— éœ€APIï¼‰
# éœ€è¦ï¼špip install transformers torch accelerate pillow

import json
import os
import time
from pathlib import Path
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ==================== é…ç½®åŒº ====================
MODEL_NAME = "Qwen/Qwen2-VL-7B-Instruct"  # æˆ– Qwen/Qwen-VL-Chat
DATA_FILE = "cogguard_bench_v2.json"
IMAGE_DIR = "images"
OUTPUT_DIR = "results/china"
SLEEP_DELAY = 0.5  # æœ¬åœ°æ¨¡å‹è¾ƒå¿«ï¼Œå¯é™ä½å»¶è¿Ÿ

# åˆ›å»ºè¾“å‡ºç›®å½•
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
print("ğŸ”„ æ­£åœ¨åŠ è½½ Qwen2-VL æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ==================== æ„å»ºæç¤ºè¯å‡½æ•° ====================
def build_prompt(current_img_path: str, ref_img_path: str, rule: str, action_plan: List[str], env: Dict, context_level: str) -> str:
    """
    æ„å»º Qwen-VL çš„çº¯æ–‡æœ¬æç¤ºï¼ˆå›¾åƒé€šè¿‡ tokenizer è‡ªåŠ¨å¤„ç†ï¼‰
    """
    text_parts = []

    if context_level in ['B', 'C', 'D', 'E']:
        text_parts.append(f"å®‰å…¨è§„åˆ™ï¼š{rule}")

    if context_level in ['C', 'D', 'E']:
        text_parts.append(f"æ­£å¸¸å‚è€ƒçŠ¶æ€ï¼ˆID: {Path(ref_img_path).stem}ï¼‰ï¼š[å›¾åƒå·²æä¾›]")

    if context_level in ['D', 'E']:
        actions_str = "\n".join([f"- {act}" for act in action_plan])
        text_parts.append(f"å¯é€‰è¡ŒåŠ¨é¢„æ¡ˆï¼š\n{actions_str}")

    if context_level == 'E':
        env_info = (
            f"ç¯å¢ƒä¿¡æ¯ï¼š\n"
            f"- ä½ç½®ï¼š{env['gps']}\n"
            f"- æ—¶é—´ï¼š{env['timestamp']}\n"
            f"- å…‰ç…§ï¼š{env['lighting']}\n"
            f"- å¤©æ°”ï¼š{env['weather']}\n"
            f"- æ˜¯å¦å·¥ä½œæ—¶é—´ï¼š{'æ˜¯' if env['is_work_hours'] else 'å¦'}\n"
            f"- ä¸Šæ¬¡å·¡é€»ï¼š{env['last_patrol_time']}"
        )
        text_parts.append(env_info)

    if not text_parts:
        text_parts.append("è¯·åˆ¤æ–­è¿™å¼ å›¾ç‰‡ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•å®‰å…¨éšæ‚£ï¼Ÿ")

    prompt = "\n\n".join(text_parts) + "\n\nè¯·é€æ­¥æ¨ç†å¹¶è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼š\n{\n  \"anomaly_detected\": true/false,\n  \"reason\": \"è¯¦ç»†è§£é‡Šï¼Œå¿…é¡»å¼•ç”¨è§„åˆ™å’Œå‚è€ƒå›¾\",\n  \"action\": \"ä»é¢„æ¡ˆä¸­é€‰æ‹©çš„å”¯ä¸€è¡ŒåŠ¨\",\n  \"confidence\": 0.0â€“1.0\n}"

    return prompt

# ==================== è®¡ç®— CRS å‡½æ•°ï¼ˆä¸å›½é™…ç‰ˆå®Œå…¨ä¸€è‡´ï¼‰====================
def compute_crs(response: Dict, ground_truth: Dict) -> float:
    accuracy = 1.0 if response.get("anomaly_detected") == ground_truth["expected_anomaly"] else 0.0

    logic_base = ground_truth["expert_logic_score"]

    reason_lower = response.get("reason", "").lower()
    env_keywords = ["time", "hour", "night", "rain", "weather", "gps", "location", "patrol", "work hours"]
    has_env_mention = any(kw in reason_lower for kw in env_keywords)

    if ground_truth["type"] in ["ambiguous_object_with_context", "false_positive_confounder", "rule_violation_with_confounding_env"]:
        if has_env_mention and response["anomaly_detected"] == ground_truth["expected_anomaly"]:
            logic_final = min(1.0, logic_base + 0.1)
        elif has_env_mention and response["anomaly_detected"] != ground_truth["expected_anomaly"]:
            logic_final = max(0.0, logic_base - 0.2)
        else:
            logic_final = logic_base
    else:
        logic_final = logic_base

    action_plan = ground_truth["action_plan"]
    predicted_action = response.get("action", "")
    action_score = 1.0 if any(predicted_action.strip() in act for act in action_plan) else 0.0

    confidence = response.get("confidence", 0.5)
    crs = (accuracy * 0.3 + logic_final * 0.5 + action_score * 0.2) * confidence
    return round(crs, 4)

# ==================== ä¸»ç¨‹åºï¼šäº”ç»„æ¶ˆèå®éªŒ ====================
def run_ablation_experiment():
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    context_levels = [
        ("A (Baseline)", False, False, False, False),
        ("B (+Text)", True, False, False, False),
        ("C (+Visual)", True, True, False, False),
        ("D (+Action)", True, True, True, False),
        ("E (+Env)", True, True, True, True)
    ]

    results = []

    for test_case in data:
        current_img_path = test_case["current_img"]
        ref_img_path = test_case["ref_img"]
        rule = test_case["rule_text"]
        actions = test_case["action_plan"]
        env = test_case["environment"]

        for level_name, use_text, use_ref, use_action, use_env in context_levels:
            try:
                print(f"[{level_name}] Processing {test_case['scene_id']}...")

                prompt = build_prompt(current_img_path, ref_img_path, rule, actions, env, level_name[0])

                # åŠ è½½å½“å‰å›¾åƒ
                current_image = Image.open(current_img_path).convert("RGB")

                # Qwen-VL è¾“å…¥æ ¼å¼ï¼š[image, text]
                inputs = tokenizer.apply_chat_template(
                    [{"role": "user", "image": current_image, "content": prompt}],
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)

                # ç”Ÿæˆå“åº”
                outputs = model.generate(inputs, max_new_tokens=512, do_sample=False)
                response_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

                # è§£æ JSON
                import re
                json_match = re.search(r'({.*})', response_text, re.DOTALL)
                if not json_match:
                    raise ValueError(f"æ— æ³•è§£æå“åº”ä¸ºJSON: {response_text[:200]}...")

                llm_output = json.loads(json_match.group(1))

                # å¼ºåˆ¶ç±»å‹è½¬æ¢
                llm_output["anomaly_detected"] = bool(llm_output.get("anomaly_detected", False))
                llm_output["confidence"] = float(llm_output.get("confidence", 0.5))
                llm_output["action"] = str(llm_output.get("action", ""))

                crs = compute_crs(llm_output, test_case)

                result = {
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": llm_output,
                    "crs": crs,
                    "raw_response": response_text,
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"],
                        "type": test_case["type"]
                    }
                }
                results.append(result)

                print(f"âœ… Success: {test_case['scene_id']} | {level_name} | CRS={crs}")
                time.sleep(SLEEP_DELAY)

            except Exception as e:
                print(f"âŒ Error on {test_case['scene_id']} ({level_name}): {e}")
                results.append({
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": {"error": str(e)},
                    "crs": 0.0,
                    "raw_response": "",
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"],
                        "type": test_case["type"]
                    }
                })

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, "results.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å›½å†…ç‰ˆå®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")

    # æ±‡æ€»ç»Ÿè®¡
    from collections import defaultdict
    crs_by_level = defaultdict(list)
    for r in results:
        crs_by_level[r["context_level"]].append(r["crs"])

    print("\nğŸ“Š å›½å†…ç‰ˆ CRSSummaryï¼š")
    for level, scores in crs_by_level.items():
        avg = sum(scores) / len(scores)
        print(f"{level}: {avg:.4f} (n={len(scores)})")

if __name__ == "__main__":
    run_ablation_experiment()