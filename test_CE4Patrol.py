import json
import os
import time
import openai
from typing import List, Dict, Any
from pathlib import Path

# ==================== é…ç½®åŒº ====================
OPENAI_API_KEY = "your-openai-api-key-here"  # â† æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
MODEL = "gpt-4o"  # æ”¯æŒ gpt-4o, gpt-4-turbo
DATA_FILE = "cogguard_bench.json"
IMAGE_DIR = "images"  # å›¾åƒå­˜æ”¾ç›®å½•
OUTPUT_DIR = "results"
SLEEP_DELAY = 1.5  # é¿å…é€Ÿç‡é™åˆ¶

# åˆ›å»ºè¾“å‡ºç›®å½•
Path(OUTPUT_DIR).mkdir(exist_ok=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
openai.api_key = OPENAI_API_KEY

# ==================== æç¤ºè¯æ„å»ºå‡½æ•° ====================
def build_prompt(current_img_path: str, ref_img_path: str, rule: str, action_plan: List[str], env: Dict) -> str:
    """
    æ„å»º CogGuard ç»“æ„åŒ–æç¤ºè¯ï¼ˆæ”¯æŒå››ç»„ä¸Šä¸‹æ–‡ï¼‰
    :param current_img_path: å½“å‰å›¾åƒè·¯å¾„
    :param ref_img_path: å‚è€ƒå›¾åƒè·¯å¾„
    :param rule: æ–‡æœ¬è§„åˆ™
    :param action_plan: è¡ŒåŠ¨é¢„æ¡ˆåˆ—è¡¨
    :param env: ç¯å¢ƒä¿¡æ¯å­—å…¸
    :return: å®Œæ•´æç¤ºå­—ç¬¦ä¸²
    """
    prompt_base = f"""ä½ æ˜¯ä¸€åå®‰å…¨å·¡é€»æœºå™¨ç‹—ï¼Œå½“å‰ä½äºã€{env['gps']}ã€‘ï¼Œæ—¶é—´ä¸ºã€{env['timestamp']}ã€‘ï¼Œå…‰ç…§ä¸ºã€{env['lighting']}ã€‘ï¼Œå¤©æ°”ä¸ºã€{env['weather']}ã€‘ã€‚

{rule}

è¯·å¯¹æ¯”å½“å‰å›¾åƒä¸ä¸‹æ–¹æ­£å¸¸å‚è€ƒå›¾ï¼ˆID: {Path(ref_img_path).stem}ï¼‰ï¼š
[æ’å…¥æ­£å¸¸å‚è€ƒå›¾]

ç°åœ¨ï¼Œè¯·é€æ­¥æ¨ç†ï¼š
1. æè¿°å›¾åƒä¸­å¯è§çš„ç‰©ä½“å’ŒçŠ¶æ€ï¼›
2. å¯¹æ¯”å½“å‰çŠ¶æ€ä¸å‚è€ƒå›¾çš„å·®å¼‚ï¼›
3. æ˜¯å¦è¿åä»»ä½•å·²çŸ¥è§„åˆ™ï¼Ÿ
4. è‹¥å­˜åœ¨å¼‚å¸¸ï¼Œç»™å‡ºå…·ä½“åŸå› ï¼›
5. æ¨èä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ˆä»ä»¥ä¸‹é¢„æ¡ˆä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€é¡¹ï¼‰ï¼š
   - {', '.join(action_plan)}

è¾“å‡ºæ ¼å¼å¿…é¡»ä¸ºä¸¥æ ¼JSONï¼Œä¸å…è®¸é¢å¤–æ–‡æœ¬ï¼š
{{
  "anomaly_detected": true/false,
  "reason": "è¯¦ç»†è§£é‡Šï¼Œå¿…é¡»å¼•ç”¨è§„åˆ™å’Œå‚è€ƒå›¾",
  "action": "ä»é¢„æ¡ˆä¸­é€‰æ‹©çš„å”¯ä¸€è¡ŒåŠ¨",
  "confidence": 0.0â€“1.0
}}
"""

    return prompt_base


# ==================== è¯„ä¼°å‡½æ•°ï¼šè®¡ç®— CRS ====================
def compute_crs(response: Dict, ground_truth: Dict) -> float:
    """
    è®¡ç®—ç»¼åˆæ¨ç†å¾—åˆ† CRS = (AccuracyÃ—0.3 + LogicÃ—0.5 + ActionÃ—0.2) Ã— Confidence
    """
    # Accuracy: æ˜¯å¦æ­£ç¡®è¯†åˆ«å¼‚å¸¸
    accuracy = 1.0 if response.get("anomaly_detected") == ground_truth["expected_anomaly"] else 0.0

    # Logic: ä¸“å®¶è¯„åˆ†ï¼ˆæ¥è‡ªæ ‡æ³¨ï¼‰
    logic = ground_truth["expert_logic_score"]

    # Action: æ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„è¡ŒåŠ¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    expected_action = ground_truth["expected_action"]
    predicted_action = response.get("action", "")
    action_score = 1.0 if any(expected_action in act for act in ground_truth["action_plan"]) and predicted_action.strip() in ground_truth["action_plan"] else 0.0

    # Confidence: æ¨¡å‹è‡ªè¯„
    confidence = response.get("confidence", 0.5)  # é»˜è®¤0.5è‹¥ç¼ºå¤±

    crs = (accuracy * 0.3 + logic * 0.5 + action_score * 0.2) * confidence
    return round(crs, 4)


# ==================== ä¸»ç¨‹åºï¼šæ‰§è¡Œå››ç»„æ¶ˆèå®éªŒ ====================
def run_ablation_experiment():
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # å››ç»„ä¸Šä¸‹æ–‡é…ç½®
    context_levels = [
        ("A (Baseline)", "", "", ""),  # ä»…å›¾åƒ + é€šç”¨é—®é¢˜
        ("B (+Text)", "{rule}", "", ""),  # + è§„åˆ™
        ("C (+Visual)", "{rule}", "{ref_img}", ""),  # + è§„åˆ™ + å‚è€ƒå›¾
        ("D (+Action)", "{rule}", "{ref_img}", "{action_plan}")  # + å…¨éƒ¨
    ]

    results = []

    for test_case in data:
        current_img = test_case["current_img"]
        ref_img = test_case["ref_img"]
        rule = test_case["rule_text"]
        actions = test_case["action_plan"]
        env = test_case["environment"]

        for level_name, rule_placeholder, ref_placeholder, action_placeholder in context_levels:
            # æ„å»ºæç¤ºè¯
            if level_name == "A (Baseline)":
                prompt = "ä½ æ˜¯ä¸€åå®‰å…¨å·¡é€»æœºå™¨ç‹—ã€‚è¯·åˆ¤æ–­è¿™å¼ å›¾ç‰‡ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•å®‰å…¨éšæ‚£ï¼Ÿè¾“å‡ºJSONæ ¼å¼ï¼š{anomaly_detected: true/false, reason: ..., action: ..., confidence: ...}"
            elif level_name == "B (+Text)":
                prompt = build_prompt("", "", rule, actions, env)
            elif level_name == "C (+Visual)":
                prompt = build_prompt("", ref_img, rule, actions, env)
            else:  # D (+Action)
                prompt = build_prompt("", ref_img, rule, actions, env)

            # è°ƒç”¨æ¨¡å‹
            try:
                print(f"[{level_name}] Processing {test_case['scene_id']}...")
                response = openai.ChatCompletion.create(
                    model=MODEL,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"file://{os.path.abspath(os.path.join(IMAGE_DIR, current_img))}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=500,
                    temperature=0.1
                )

                content = response.choices[0].message.content.strip()

                # è§£æ JSON
                import re
                json_match = re.search(r'({.*})', content, re.DOTALL)
                if not json_match:
                    raise ValueError("æ— æ³•è§£ææ¨¡å‹è¾“å‡ºä¸ºJSON")

                llm_output = json.loads(json_match.group(1))

                # è®¡ç®—CRS
                crs = compute_crs(llm_output, test_case)

                result = {
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": llm_output,
                    "crs": crs,
                    "raw_response": content,
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"]
                    }
                }
                results.append(result)

                print(f"âœ… Success: {test_case['scene_id']} | CRS={crs}")
                time.sleep(SLEEP_DELAY)

            except Exception as e:
                print(f"âŒ Error on {test_case['scene_id']} ({level_name}): {e}")
                results.append({
                    "scene_id": test_case["scene_id"],
                    "context_level": level_name,
                    "model_response": {"error": str(e)},
                    "crs": 0.0,
                    "raw_response": "",
                    "ground_truth": {
                        "expected_anomaly": test_case["expected_anomaly"],
                        "expected_action": test_case["expected_action"],
                        "expert_logic_score": test_case["expert_logic_score"]
                    }
                })

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, "results.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")

    # æ±‡æ€»ç»Ÿè®¡
    from collections import defaultdict
    crs_by_level = defaultdict(list)
    for r in results:
        crs_by_level[r["context_level"]].append(r["crs"])

    print("\nğŸ“Š ç»¼åˆCRSç»Ÿè®¡ï¼š")
    for level, scores in crs_by_level.items():
        avg = sum(scores) / len(scores)
        print(f"{level}: {avg:.4f} (n={len(scores)})")


if __name__ == "__main__":
    run_ablation_experiment()